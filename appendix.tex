\chapter{Appendix}

\section{Expression Transformation}
\label{sec:ap-lemming-expressions}

This leads to unbalanced trees since the left subtree of a node might be a complex expression while the right node is always a single function. We name this a left deep tree. Hence, $\s{refinement}$ is complete if it is possible that every binary expression tree over $\s{operators}$ can be represented as such a left deep tree. 

As described in Section~\ref{sec:learning-invariants}, our refinement operator $\s{refinement}$ is complete since it is able to generate Taylor series.\footnote{Our definition of completeness covers global completeness. We do not claim that our refinement operator offers local completeness~\cite{Muggleton1994}.} These series can approximate other expressions~\cite{Weltner2008}.
However, a lot of expressions can be directly transformed into a left-deep tree.
Let $\s{expression}_1$ and $\s{expression}_2$ be two expressions which are connected by a binary operation $\s{wildOperator}$ which can be any of the binary operations in $\s{operators}$. Their binary expression tree is shown in Figure~\ref{fig:hobbit-lemming-tree-a}. If $\s{expression}_2$ is a single function the tree already has the form of a left-deep tree. If $\s{expression}_2$ comprises more than a single function, it can be represented by its two subexpressions $\s{expression}_{2,1}$ and $\s{expression}_{2,2}$ that are connected by the operator $\s{operator} \in \s{operators}$ as follows:
\begin{equation}
\s{expression}_2 = \s{expression}_{2,1} \s{operator} \s{expression}_{2,2}\,.
\end{equation}
As shown in Figure~\ref{fig:hobbit-lemming-tree-b}, the binary expression tree of the two connected expressions $\s{expression}_1$ and $\s{expression}_2$ is not a left-deep tree:
\begin{equation}
\s{expression}_1 \s{wildOperator} \s{expression}_2 = \s{expression}_1 \s{wildOperator} (\s{expression}_{2,1} \s{operator} \s{expression}_{2,2})\,.
\end{equation}
In a left-deep tree, the left operator has to be executed before the right operator. However, because of the parenthesis in the expression, this does not hold in our example.

\speedupfigure{
\input{figures/lemming-trees}
}{fig:hobbit-lemming-tree}

Since $|\s{operators}|=4$, there are 16 different combinations that the two operators $\s{wildOperator}$ and $\s{operator}$ can have in the equation above.
We demonstrate for 14 of the 16 combinations how the expression can be transformed to create a left-deep tree. In the following, we use $\pm$ in cases in which plus or minus could be used:
\begin{align}
% + +
\s{expression}_1 + (\s{expression}_{2,1} + \s{expression}_{2,2}) &= (\s{expression}_1 + \s{expression}_{2,1}) + \s{expression}_{2,2}\,,\\
% - - 
\s{expression}_1 - (\s{expression}_{2,1} - \s{expression}_{2,2}) &= (\s{expression}_1 - \s{expression}_{2,1}) + \s{expression}_{2,2}\,,\\
% x x
\s{expression}_1 \times (\s{expression}_{2,1} \times \s{expression}_{2,2}) &= (\s{expression}_1 \times \s{expression}_{2,1}) \times \s{expression}_{2,2}\,,\\
% / /
\s{expression}_1 / (\s{expression}_{2,1} / \s{expression}_{2,2}) &= (\s{expression}_1 / \s{expression}_{2,1}) \times \s{expression}_{2,2}\,,\\
% + -
\s{expression}_1 + (\s{expression}_{2,1} - \s{expression}_{2,2}) &= (\s{expression}_1 + \s{expression}_{2,1}) - \s{expression}_{2,2}\,,\\
% - +
\s{expression}_1 - (\s{expression}_{2,1} + \s{expression}_{2,2}) &= (\s{expression}_1 - \s{expression}_{2,1}) - \s{expression}_{2,2}\,,\\
% x /
\s{expression}_1 \times (\s{expression}_{2,1} / \s{expression}_{2,2}) &= (\s{expression}_1 \times \s{expression}_{2,1}) / \s{expression}_{2,2}\,,\\
% / x
\s{expression}_1 / (\s{expression}_{2,1} \times \s{expression}_{2,2}) &= (\s{expression}_1 / \s{expression}_{2,1}) / \s{expression}_{2,2}\,,\\
% +- x
\begin{split}
\s{expression}_1 \pm (\s{expression}_{2,1} \times \s{expression}_{2,2}) &= ((\s{expression}_1 \times \s{expression}_{2,2}) / \s{expression}_{2,2}) \pm (\s{expression}_{2,1} \times \s{expression}_{2,2})\\
&=((\s{expression}_1 / \s{expression}_{2,2}) \pm \s{expression}_{2,1}) \times \s{expression}_{2,2}\,,
\end{split}\\
% +- /
\begin{split}
\s{expression}_1 \pm (\s{expression}_{2,1} / \s{expression}_{2,2}) &= ((\s{expression}_1 \times \s{expression}_{2,2}) / \s{expression}_{2,2}) \pm (\s{expression}_{2,1} / \s{expression}_{2,2})\\
&=((\s{expression}_1 \times \s{expression}_{2,2}) \pm \s{expression}_{2,1}) / \s{expression}_{2,2}\,,
\end{split}\\
% x +-
\label{eq:hobbit-example-transformation}
\begin{split}
\s{expression}_1 \times (\s{expression}_{2,1} \pm \s{expression}_{2,2}) &= (\s{expression}_1 \times \s{expression}_{2,1}) \pm (\s{expression}_1 \times\s{expression}_{2,2}) \\
 &= ((\s{expression}_1 \times \s{expression}_{2,1}) \times (\s{expression}_{2,2} / \s{expression}_{2,2})) \pm (\s{expression}_1 \times \s{expression}_{2,2}) \\
 &= (((\s{expression}_1 \times \s{expression}_{2,1}) / \s{expression}_{2,2}) \pm \s{expression}_1)\times \s{expression}_{2,2})\,.
\end{split}
\end{align}


\begin{comment}
\label{sec:learning-invariants}

It is possible to transform many expressions directly into a left deep tree. Section~\ref{sec:ap-lemming-expressions} lists several basic examples. 
However, since our refinement operator is able to generate polynomial expressions, it can create Taylor series that approximate expressions that cannot be transformed directly into a left deep tree~\cite{Weltner2008}.\footnote{Note that for some expressions the Taylor series does not converge~\cite{Weltner2008}. Hence, the refinement operator would have to generate an infinite long polynomial to approximate such an expression.}

\speedupfigure{
\input{figures/lemming-trees}
}{fig:hobbit-lemming-tree}

Let $\s{operator}$ be a binary operator with $\s{operator} \in \s{operators}$ and let $\s{inverseOperator}$ be it's inverse, i.e.,
\begin{equation}
(\s{expression} \s{operator} \s{function}_{\s{i}}) \s{inverseOperator} \s{function}_{\s{i}} = \s{expression}\,.
\end{equation}
%We define a left deep binary expression tree as binary expression tree in which each node is either an element of $\s{functions}$ or has only elements of $\s{functions}$ as right children. 
Further, it should be noted that all binary arithmetic operations in $\s{operators}$ have an inverse operation in $\s{operators}$. Each binary expression tree over $\s{operators}$ can be transformed into a left deep tree by repeatedly applying the transformation step depicted in Figure~\ref{fig:hobbit-lemming-tree}. 
The first tree in Figure~\ref{fig:hobbit-lemming-tree-a} shows two expressions $\s{expression}_1$ and $\s{expression}_2$. They are connected by a binary operation $\s{wildOperator}$ which can be any of the binary operations in $\s{operators}$. If $\s{expression}_2$ is a single function the tree already has the form of a left deep tree. If $\s{expression}_2$ comprises more than a single function, it can be represented by its two subexpressions $\s{expression}_{2,1}$ and $\s{expression}_{2,2}$ that are connected by the operator $\s{operator}$ as follows:
\begin{equation}
\s{expression}_2 = \s{expression}_{2,1} \s{operator} \s{expression}_{2,2}\,.
\end{equation}
This representation is depicted in Figure~\ref{fig:hobbit-lemming-tree-b}.
By utilizing the inverse operator $\s{inverseOperator}$, we can transform the tree into an equivalent tree that is depicted in Figure~\ref{fig:hobbit-lemming-tree-c}. We can apply this transformation since the following holds for all operations in $\s{operators}$:
\begin{equation}
\s{expression}_1 \s{wildOperator} (\s{expression}_{2,1} \s{operator} \s{expression}_{2,2}) = (((\s{expression}_1 \s{inverseOperator} \s{expression}_{2,2}) \s{wildOperator} \s{expression}_{2,1}) \s{operator} \s{expression}_{2,2})\,.
\end{equation}
\itodo{Check this proof. It seems to be flawed. At least for $\s{expression}_1 \times (\s{expression}_{2,1} + \s{expression}_{2,2})$ the result seems to be $(((\s{expression}_1 \times \s{expression}_{2,1})/\s{expression}_{2,2})+\s{expression}_1)\times \s{expression}_{2,2}$}
\itodo{proof problem start}
Case 1: both operators are the same. Addition and multiplication are associative. Hence, the solution is trivial:
\begin{align}
\s{expression}_1 + (\s{expression}_{2,1} + \s{expression}_{2,2}) &= (\s{expression}_1 + \s{expression}_{2,1}) + \s{expression}_{2,2}\,.
\end{align}
Subtraction and division can be solved as follows:
\begin{align}
\s{expression}_1 - (\s{expression}_{2,1} - \s{expression}_{2,2}) &= (\s{expression}_1 - \s{expression}_{2,1}) + \s{expression}_{2,2}\,,\\
\s{expression}_1 / (\s{expression}_{2,1} / \s{expression}_{2,2}) &= (\s{expression}_1 / \s{expression}_{2,1}) * \s{expression}_{2,2}\,.
\end{align}

Case 2: both operators are different, but have the same "priority". The two cases for addition and subtraction are as follows:
\begin{align}
\s{expression}_1 + (\s{expression}_{2,1} - \s{expression}_{2,2}) &= (\s{expression}_1 + \s{expression}_{2,1}) - \s{expression}_{2,2}\,,\\
\s{expression}_1 - (\s{expression}_{2,1} + \s{expression}_{2,2}) &= (\s{expression}_1 - \s{expression}_{2,1}) - \s{expression}_{2,2}\,.
\end{align}
The two cases for multiplication and division are as follows:
\begin{align}
\s{expression}_1 * (\s{expression}_{2,1} / \s{expression}_{2,2}) &= (\s{expression}_1 * \s{expression}_{2,1}) / \s{expression}_{2,2}\,,\\
\s{expression}_1 / (\s{expression}_{2,1} * \s{expression}_{2,2}) &= (\s{expression}_1 / \s{expression}_{2,1}) / \s{expression}_{2,2}\,.
\end{align}

Case 3: both operators are different and have different priorities.
\begin{align}
\begin{split}
\s{expression}_1 \pm (\s{expression}_{2,1} \times \s{expression}_{2,2}) &= (\s{expression}_1 \times \s{expression}_{2,2}) / \s{expression}_{2,2} \pm (\s{expression}_{2,1} \times \s{expression}_{2,2}\\
&=((\s{expression}_1 / \s{expression}_{2,2}) \pm \s{expression}_{2,1}) \times \s{expression}_{2,2}\,,
\end{split}\\
\begin{split}
\s{expression}_1 \pm (\s{expression}_{2,1} / \s{expression}_{2,2}) &= (\s{expression}_1 \times \s{expression}_{2,2}) / \s{expression}_{2,2} \pm (\s{expression}_{2,1} / \s{expression}_{2,2}\\
&=((\s{expression}_1 \times \s{expression}_{2,2}) \pm \s{expression}_{2,1}) / \s{expression}_{2,2}\,,
\end{split}\\
\begin{split}
\s{expression}_1 \times (\s{expression}_{2,1} \pm \s{expression}_{2,2}) &= \s{expression}_1 \times \s{expression}_{2,1} \pm \s{expression}_1 \times\s{expression}_{2,2} \\
 &= (((\s{expression}_1 \times \s{expression}_{2,1}) / \s{expression}_{2,2}) \pm \s{expression}_1)\s{expression}_{2,2})\,,
\end{split}\\
\s{expression}_1 * (\s{expression}_{2,1} - \s{expression}_{2,2}) &= \,,\\
\s{expression}_1 / (\s{expression}_{2,1}  \s{expression}_{2,2}) &= \,,\\
\begin{split}
\s{expression}_1 / (\s{expression}_{2,1} \pm \s{expression}_{2,2}) &= \s{expression}_1 / (((\s{expression}_{2,1} / \s{expression}_{2,2}) \pm \s{expression}_{2,2}) \times \s{expression}_{2,2})\,.
\end{split}
\end{align}

\itodo{proof problem end}
Let $|\s{expression}|$ be the number of arithmetic operators in $\s{expression}$. In a left deep tree, all right children have the length 0 because they comprise functions without any arithmetic operators.
After applying the transformation step described above, the overall length of the expression has been increased. However, all visible right child nodes in Figure~\ref{fig:hobbit-lemming-tree-c} contain shorter expressions than the right child node in Figure~\ref{fig:hobbit-lemming-tree-a} because $|\s{expression}_{2}| = |\s{expression}_{2,1}| + |\s{expression}_{2,2}| + 1$. It follows by virtue of induction that repeating the transformation step further reduces the lengths of expressions in right child nodes until all right child nodes contain single functions. This proves that all binary expression trees can be transformed into a left deep tree representation. %$\qed$
\end{comment}

\section{Lemming Error Plots}
\label{sec:ap-lemming-error-plots}

Figures~\ref{fig:hobbit-lemming-lgd-error} and~\ref{fig:hobbit-lemming-icc-error} show the error scores for the \gls{lgd} and \gls{icc} dataset, respectively. The diagrams show the reduction of the error scores throughout the experiment described in Section~\ref{sec:ho-lemming-eval}. Each diagram comprises three curves for the three runs.

\speedupfigure{
\input{figures/lemming-lgd-error}
}{fig:hobbit-lemming-lgd-error}

\speedupfigure{
\input{figures/lemming-icc-error}
}{fig:hobbit-lemming-icc-error}

\clearpage% Flush page

\section{Detailed Correlation Results}
\label{sec:ap-detailed-correlation-results}

Tables~\ref{tab:se-detailed-results1} and~\ref{tab:se-detailed-results1} show the detailed results of the new coherence measures and coherence measures that have been proposed by related work. The description of the latter can be found ins Section~\ref{sec:se-existing-measures}. The experiment description can be found in Section~\ref{sec:se-palmetto-evaluation}. The discussion of the most important results can be found in Section~\ref{sec:se-discussion}.

\begin{comment}
\begin{sidewaysfigure}
\centering
%\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lccccccccccccccc@{}}
\toprule
Name & {$\s{cohV}$} & {$\s{cohP}$} & {$\s{cohOneAny}$} & {$\s{cohAnyAny}$} & {$\s{cohOneAll}$} & {$\s{cohUMass}$} & {$\s{cohUCI}$} & {$\s{cohNPMI}$} & {$\s{cohShogenji}$} & {$\s{cohOlsson}$} & {$\s{cohFitelson}$} & {$\s{cohCos}$} & {$\s{cohDice}$} & {$\s{cohJac}$} & {$\s{cohCen}$} \\
$\s{segmentations}$ & $\s{sOneAll}$ & $\s{sOnePre}$ & $\s{sOneAny}$ & $\s{sAnyAny}$ & $\s{sOneAll}$ & $\s{sOnePre}$ & $\s{sOneOne}$ & $\s{sOneOne}$ & $\s{sAllOne}$ & $\s{sSetSet}$ & $\s{sOneAny}$ & $\s{sOneOne}$ & $\s{sOneOne}$ & $\s{sOneOne}$ & $\s{sOneSet}$ \\
$\s{probEstimations}$ & $\sWindow{110}$ & $\sWindow{70}$ & $\s{booleanDocument}$ & $\s{booleanDocument}$ & $\s{booleanDocument}$ & $\s{booleanDocument}$ & $\sWindow{10}$ & $\sWindow{10}$ & $\s{booleanDocument}$ & $\sWindow{300}$ & $\sWindow{300}$ & $\cWindow{5}$ & $\cWindow{5}$ & $\cWindow{5}$ & $\cWindow{5}$ \\
$\s{confirmationMeasures}$ & $\s{indirectConfMeasure}_{\s{cos},\s{measureNormLogRatio},1}$ & $\s{measureFitelson}$ & $\s{measureDifference}$ & $\s{measureDifference}$ & $\s{measureDifference}$ & $\s{measureLogConditional}$ & $\s{measureLogRatio}$ & $\s{measureNormLogRatio}$ & $\s{measureLogShogenji}$ & $\s{measureOlsson}$ & $\s{measureFitelson}$ & $\s{indirectConfMeasure}_{\s{cos},\s{measureNormLogRatio},1}$ & $\s{indirectConfMeasure}_{\s{dice},\s{measureNormLogRatio},2}$ & $\s{indirectConfMeasure}_{\s{jac},\s{measureNormLogRatio},2}$ & $\s{indirectConfMeasure}_{\s{cos},\s{measureNormLogRatio},1}$ \\
$\s{aggregations}$ & $\s{aggArith}$ & $\s{aggArith}$ & $\s{aggArith}$ & $\s{aggArith}$ & $\s{aggArith}$ & $\s{aggArith}$ & $\s{aggArith}$ & $\s{aggArith}$ & $\s{aggArith}$ & $\s{cdot}$ & $\s{aggArith}$ & $\s{aggArith}$ & $\s{aggArith}$ & $\s{aggArith}$ & $\s{aggArith}$ \\
\midrule
\multirow{2}{*}{20NG} &
\multicolumn{1}{c}{0.832} & \multicolumn{1}{c}{0.825} & \multicolumn{1}{c}{0.822} & \multicolumn{1}{c}{0.765} & \multicolumn{1}{c}{0.599} & \multicolumn{1}{c}{0.555} & \multicolumn{1}{c}{0.747} & \multicolumn{1}{c}{0.809} & \multicolumn{1}{c}{-0.540} & \multicolumn{1}{c}{0.185} & \multicolumn{1}{c}{0.729} & \multicolumn{1}{c}{0.790} & \multicolumn{1}{c}{0.740} & \multicolumn{1}{c}{0.738} & \multicolumn{1}{c}{0.765} \\
& (691.5) & (1810.0) & (2090.0) & (12681.0) & (76170.0) & (95887.0) & (17055.0) & (4107.0) & (447307.0) & (220157.5) & (21238.0) & (7555.0) & (18817.0) & (21209.0) & (12579.0) \\
\multirow{2}{*}{Genomics} &
\multicolumn{1}{c}{0.726} & \multicolumn{1}{c}{0.721} & \multicolumn{1}{c}{0.452} & \multicolumn{1}{c}{0.357} & \multicolumn{1}{c}{-0.042} & \multicolumn{1}{c}{0.461} & \multicolumn{1}{c}{0.602} & \multicolumn{1}{c}{0.671} & \multicolumn{1}{c}{-0.351} & \multicolumn{1}{c}{NaN} & \multicolumn{1}{c}{0.123} & \multicolumn{1}{c}{0.640} & \multicolumn{1}{c}{0.641} & \multicolumn{1}{c}{0.633} & \multicolumn{1}{c}{0.654} \\
& (3147.5) & (2896.0) & (50037.0) & (89345.0) & (341280.5) & (47398.0) & (20427.5) & (9804.0) & (422975.0) & (501449.0) & (238312.0) & (14587.5) & (14479.5) & (20499.0) & (12671.0) \\
\multirow{2}{*}{NYT} &
\multicolumn{1}{c}{0.820} & \multicolumn{1}{c}{0.757} & \multicolumn{1}{c}{0.612} & \multicolumn{1}{c}{0.575} & \multicolumn{1}{c}{0.447} & \multicolumn{1}{c}{0.519} & \multicolumn{1}{c}{0.751} & \multicolumn{1}{c}{0.798} & \multicolumn{1}{c}{-0.334} & \multicolumn{1}{c}{NaN} & \multicolumn{1}{c}{0.657} & \multicolumn{1}{c}{0.733} & \multicolumn{1}{c}{0.650} & \multicolumn{1}{c}{0.659} & \multicolumn{1}{c}{0.724} \\
& (11.5) & (2215.0) & (22738.0) & (31523.0) & (91691.0) & (50798.0) & (2641.0) & (267.0) & (389079.0) & (290627.5) & (13485.0) & (4082.5) & (14881.0) & (8417.0) & (5009.0) \\
\multirow{2}{*}{RTL-NYT} &
\multicolumn{1}{c}{0.736} & \multicolumn{1}{c}{0.720} & \multicolumn{1}{c}{0.438} & \multicolumn{1}{c}{0.406} & \multicolumn{1}{c}{0.335} & \multicolumn{1}{c}{0.099} & \multicolumn{1}{c}{0.544} & \multicolumn{1}{c}{0.659} & \multicolumn{1}{c}{0.487} & \multicolumn{1}{c}{0.236} & \multicolumn{1}{c}{0.629} & \multicolumn{1}{c}{0.630} & \multicolumn{1}{c}{0.683} & \multicolumn{1}{c}{0.665} & \multicolumn{1}{c}{0.615} \\
& (151.5) & (886.0) & (114219.0) & (130845.0) & (178236.0) & (316872.0) & (60306.0) & (14433.0) & (148803.0) & (218296.5) & (24643.0) & (23859.0) & (7749.5) & (44337.0) & (30191.0) \\
\multirow{2}{*}{RTL-Wiki} &
\multicolumn{1}{c}{0.684} & \multicolumn{1}{c}{0.645} & \multicolumn{1}{c}{0.499} & \multicolumn{1}{c}{0.459} & \multicolumn{1}{c}{0.438} & \multicolumn{1}{c}{0.336} & \multicolumn{1}{c}{0.548} & \multicolumn{1}{c}{0.609} & \multicolumn{1}{c}{0.321} & \multicolumn{1}{c}{0.156} & \multicolumn{1}{c}{0.608} & \multicolumn{1}{c}{0.579} & \multicolumn{1}{c}{0.565} & \multicolumn{1}{c}{0.535} & \multicolumn{1}{c}{0.571} \\
& (144.5) & (3901.0) & (52244.0) & (74832.0) & (90708.0) & (164631.0) & (28928.0) & (9725.0) & (106464.0) & (266499.5) & (10085.0) & (18203.0) & (22587.0) & (23730.0) & (20812.0) \\
\multirow{2}{*}{Movie} &
\multicolumn{1}{c}{0.542} & \multicolumn{1}{c}{0.533} & \multicolumn{1}{c}{0.454} & \multicolumn{1}{c}{0.408} & \multicolumn{1}{c}{0.489} & \multicolumn{1}{c}{0.143} & \multicolumn{1}{c}{0.447} & \multicolumn{1}{c}{0.452} & \multicolumn{1}{c}{0.351} & \multicolumn{1}{c}{0.285} & \multicolumn{1}{c}{0.494} & \multicolumn{1}{c}{0.465} & \multicolumn{1}{c}{0.474} & \multicolumn{1}{c}{0.458} & \multicolumn{1}{c}{0.467} \\
& (11963.5) & (16355.0) & (59269.0) & (85740.0) & (37356.0) & (345189.0) & (63203.0) & (60316.0) & (146733.0) & (109348.5) & (35381.0) & (51831.5) & (46326.0) & (28700.0) & (50491.0) \\
\midrule
Avg. rank & 2685.0 & 4677.2 & 50099.5 & 70827.7 & 135906.9 & 170129.2 & 32093.4 & 16442.0 & 276893.5 & 267729.8 & 57190.7 & 20019.8 & 20806.7 & 24482.0 & 21958.8 \\
Std. dev. & 4287.6 & 5304.1 & 34769.6 & 38964.2 & 101035.3 & 120432.8 & 22373.6 & 20132.6 & 144546.0 & 118983.1 & 81406.1 & 15646.6 & 12274.6 & 10779.9 & 14970.6 \\
\bottomrule
\end{tabular}
%}
\end{sidewaysfigure}
\end{comment}

\begin{sidewaysfigure}
\captionof{table}[Correlations and rankings of coherence measures; part 1/2.]{Correlations and rankings (in paranthesis) of the coherence measures $\s{cohV}$, $\s{cohP}$ and the measures listed in Section~\ref{sec:se-existing-measures} except the measures proposed by Rosner~\etal~\cite{rosner2014} and Aletras~\etal~\cite{aletras2013}.}
\label{tab:se-detailed-results1}
\centering
%\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lccccccccccc@{}}
\toprule
Name & {$\s{cohV}$} & {$\s{cohP}$} & {$\s{cohUMass}$} & {$\s{cohUCI}$} & {$\s{cohNPMI}$} & {$\s{cohShogenji}$} & {$\s{cohOlsson}$} & {$\s{cohFitelson}$} \\
$\s{segmentations}$ & $\s{sOneAll}$ & $\s{sOnePre}$ & $\s{sOnePre}$ & $\s{sOneOne}$ & $\s{sOneOne}$ & $\s{sAllOne}$ & $\s{sSetSet}$ & $\s{sOneAny}$ \\
$\s{probEstimations}$ & $\sWindow{110}$ & $\sWindow{70}$ & $\s{booleanDocument}$ & $\sWindow{10}$ & $\sWindow{10}$ & $\s{booleanDocument}$ & $\sWindow{300}$ & $\sWindow{300}$ \\
$\s{confirmationMeasures}$ & $\s{indirectConfMeasure}_{\s{cos},\s{measureNormLogRatio},1}$ & $\s{measureFitelson}$ & $\s{measureLogConditional}$ & $\s{measureLogRatio}$ & $\s{measureNormLogRatio}$ & $\s{measureLogShogenji}$ & $\s{measureOlsson}$ & $\s{measureFitelson}$ \\
$\s{aggregations}$ & $\s{aggArith}$ & $\s{aggArith}$ & $\s{aggArith}$ & $\s{aggArith}$ & $\s{aggArith}$ & $\s{aggArith}$ & $\s{cdot}$ & $\s{aggArith}$ \\
\midrule
\multirow{2}{*}{20NG} &
\multicolumn{1}{c}{0.832} & \multicolumn{1}{c}{0.825} & \multicolumn{1}{c}{0.555} & \multicolumn{1}{c}{0.747} & \multicolumn{1}{c}{0.809} & \multicolumn{1}{c}{-0.540} & \multicolumn{1}{c}{0.185} & \multicolumn{1}{c}{0.729} \\
& (691.5) & (1\,810.0) & (95\,887.0) & (17\,055.0) & (4\,107.0) & (447\,307.0) & (220\,157.5) & (21\,238.0) \\
\multirow{2}{*}{Genomics} &
\multicolumn{1}{c}{0.726} & \multicolumn{1}{c}{0.721} & \multicolumn{1}{c}{0.461} & \multicolumn{1}{c}{0.602} & \multicolumn{1}{c}{0.671} & \multicolumn{1}{c}{-0.351} & \multicolumn{1}{c}{NaN} & \multicolumn{1}{c}{0.123} \\
& (3\,147.5) & (2\,896.0) & (47\,398.0) & (20\,427.5) & (9\,804.0) & (422\,975.0) & (501\,449.0) & (238\,312.0) \\
\multirow{2}{*}{NYT} &
\multicolumn{1}{c}{0.820} & \multicolumn{1}{c}{0.757} & \multicolumn{1}{c}{0.519} & \multicolumn{1}{c}{0.751} & \multicolumn{1}{c}{0.798} & \multicolumn{1}{c}{-0.334} & \multicolumn{1}{c}{NaN} & \multicolumn{1}{c}{0.657} \\
& (11.5) & (2\,215.0) & (50\,798.0) & (2\,641.0) & (267.0) & (389\,079.0) & (290\,627.5) & (13\,485.0) \\
\multirow{2}{*}{RTL-NYT} &
\multicolumn{1}{c}{0.736} & \multicolumn{1}{c}{0.720} & \multicolumn{1}{c}{0.099} & \multicolumn{1}{c}{0.544} & \multicolumn{1}{c}{0.659} & \multicolumn{1}{c}{0.487} & \multicolumn{1}{c}{0.236} & \multicolumn{1}{c}{0.629} \\
& (151.5) & (886.0) & (316\,872.0) & (60\,306.0) & (14\,433.0) & (148\,803.0) & (218\,296.5) & (24\,643.0) \\
\multirow{2}{*}{RTL-Wiki} &
\multicolumn{1}{c}{0.684} & \multicolumn{1}{c}{0.645} & \multicolumn{1}{c}{0.336} & \multicolumn{1}{c}{0.548} & \multicolumn{1}{c}{0.609} & \multicolumn{1}{c}{0.321} & \multicolumn{1}{c}{0.156} & \multicolumn{1}{c}{0.608} \\
& (144.5) & (3\,901.0) & (164\,631.0) & (28\,928.0) & (9\,725.0) & (106\,464.0) & (266\,499.5) & (10\,085.0) \\
\multirow{2}{*}{Movie} &
\multicolumn{1}{c}{0.542} & \multicolumn{1}{c}{0.533} & \multicolumn{1}{c}{0.143} & \multicolumn{1}{c}{0.447} & \multicolumn{1}{c}{0.452} & \multicolumn{1}{c}{0.351} & \multicolumn{1}{c}{0.285} & \multicolumn{1}{c}{0.494} \\
& (11\,963.5) & (16\,355.0) & (345\,189.0) & (63\,203.0) & (60\,316.0) & (146\,733.0) & (109\,348.5) & (35\,381.0) \\
\midrule
Avg. rank & 2\,685.0 & 4\,677.2 & 170\,129.2 & 32\,093.4 & 16\,442.0 & 276\,893.5 & 267\,729.8 & 57\,190.7 \\
Std. dev. & 4\,287.6 & 5\,304.1 & 120\,432.8 & 22\,373.6 & 20\,132.6 & 144\,546.0 & 118\,983.1 & 81\,406.1 \\
\bottomrule
\end{tabular}
%}
\end{sidewaysfigure}

\begin{sidewaysfigure}
\centering
\captionof{table}[Correlations and rankings of coherence measures; part 2/2.]{Correlations and rankings (in paranthesis) of the coherence measures proposed by Rosner~\etal~\cite{rosner2014} and Aletras~\etal~\cite{aletras2013}.}
\label{tab:se-detailed-results2}
%\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lccccccc@{}}
\toprule
Name & {$\s{cohOneAny}$} & {$\s{cohAnyAny}$} & {$\s{cohOneAll}$} & {$\s{cohCos}$} & {$\s{cohDice}$} & {$\s{cohJac}$} & {$\s{cohCen}$} \\
$\s{segmentations}$ & $\s{sOneAll}$ & $\s{sOnePre}$ & $\s{sOneAny}$ & $\s{sOneOne}$ & $\s{sOneOne}$ & $\s{sOneOne}$ & $\s{sOneSet}$ \\
$\s{probEstimations}$ & $\s{booleanDocument}$ & $\s{booleanDocument}$ & $\s{booleanDocument}$ & $\cWindow{5}$ & $\cWindow{5}$ & $\cWindow{5}$ & $\cWindow{5}$ \\
$\s{confirmationMeasures}$ & $\s{measureDifference}$ & $\s{measureDifference}$ & $\s{measureDifference}$ & $\s{indirectConfMeasure}_{\s{cos},\s{measureNormLogRatio},1}$ & $\s{indirectConfMeasure}_{\s{dice},\s{measureNormLogRatio},2}$ & $\s{indirectConfMeasure}_{\s{jac},\s{measureNormLogRatio},2}$ & $\s{indirectConfMeasure}_{\s{cos},\s{measureNormLogRatio},1}$ \\
$\s{aggregations}$ & $\s{aggArith}$ & $\s{aggArith}$ & $\s{aggArith}$ & $\s{aggArith}$ & $\s{aggArith}$ & $\s{aggArith}$ & $\s{aggArith}$ \\
\midrule
\multirow{2}{*}{20NG} &
\multicolumn{1}{c}{0.822} & \multicolumn{1}{c}{0.765} & \multicolumn{1}{c}{0.599} & \multicolumn{1}{c}{0.790} & \multicolumn{1}{c}{0.740} & \multicolumn{1}{c}{0.738} & \multicolumn{1}{c}{0.765} \\
& (2\,090.0) & (12\,681.0) & (76\,170.0) & (7\,555.0) & (18\,817.0) & (21\,209.0) & (12\,579.0) \\
\multirow{2}{*}{Genomics} &
\multicolumn{1}{c}{0.452} & \multicolumn{1}{c}{0.357} & \multicolumn{1}{c}{-0.042} & \multicolumn{1}{c}{0.640} & \multicolumn{1}{c}{0.641} & \multicolumn{1}{c}{0.633} & \multicolumn{1}{c}{0.654} \\
& (50\,037.0) & (89\,345.0) & (341\,280.5) & (14\,587.5) & (14\,479.5) & (20\,499.0) & (12\,671.0) \\
\multirow{2}{*}{NYT} &
\multicolumn{1}{c}{0.612} & \multicolumn{1}{c}{0.575} & \multicolumn{1}{c}{0.447} & \multicolumn{1}{c}{0.733} & \multicolumn{1}{c}{0.650} & \multicolumn{1}{c}{0.659} & \multicolumn{1}{c}{0.724} \\
& (22\,738.0) & (31\,523.0) & (91\,691.0) & (4\,082.5) & (14\,881.0) & (8\,417.0) & (5\,009.0) \\
\multirow{2}{*}{RTL-NYT} &
\multicolumn{1}{c}{0.438} & \multicolumn{1}{c}{0.406} & \multicolumn{1}{c}{0.335} & \multicolumn{1}{c}{0.630} & \multicolumn{1}{c}{0.683} & \multicolumn{1}{c}{0.665} & \multicolumn{1}{c}{0.615} \\
& (114\,219.0) & (130\,845.0) & (178\,236.0) & (23\,859.0) & (7\,749.5) & (44\,337.0) & (30\,191.0) \\
\multirow{2}{*}{RTL-Wiki} &
\multicolumn{1}{c}{0.499} & \multicolumn{1}{c}{0.459} & \multicolumn{1}{c}{0.438} & \multicolumn{1}{c}{0.579} & \multicolumn{1}{c}{0.565} & \multicolumn{1}{c}{0.535} & \multicolumn{1}{c}{0.571} \\
& (52\,244.0) & (74\,832.0) & (90\,708.0) & (18\,203.0) & (22\,587.0) & (23\,730.0) & (20\,812.0) \\
\multirow{2}{*}{Movie} &
\multicolumn{1}{c}{0.454} & \multicolumn{1}{c}{0.408} & \multicolumn{1}{c}{0.489} & \multicolumn{1}{c}{0.465} & \multicolumn{1}{c}{0.474} & \multicolumn{1}{c}{0.458} & \multicolumn{1}{c}{0.467} \\
& (59\,269.0) & (85\,740.0) & (37\,356.0) & (51\,831.5) & (46\,326.0) & (28\,700.0) & (50\,491.0) \\
\midrule
Avg. rank & 50\,099.5 & 70\,827.7 & 135\,906.9 & 20\,019.8 & 20\,806.7 & 24\,482.0 & 21\,958.8 \\
Std. dev. & 34\,769.6 & 38\,964.2 & 101\,035.3 & 15\,646.6 & 12\,274.6 & 10\,779.9 & 14\,970.6 \\
\bottomrule
\end{tabular}
%}
\end{sidewaysfigure}

\clearpage% Flush page

\section{Detailed Measure Comparison}
\label{sec:ap-lodcat-small-corpus}

In an additional experiment, we compare the coherence-based evaluation of topic models with the measures proposed by Griffiths~\etal~\cite{Griffiths2004} and Arun~\etal~\cite{Arun2010}, which are explained in detail in Section~\ref{sec:pre-numberOfTopics}.
First, we create a subset of the Wikipedia corpus comprising 337\,754 documents with 236 million word tokens and 4.4 million different word types.
In a first step, we generate topic models with $\s{numberOfTopics}=\{50,100,150,200,250,300,400\}$ and measure their coherence. We measure the coherence of the generated models using the $\s{cohP}$ measure. Then, we generate additional models with different numbers of topics in areas in which the coherence seems to be high. This holds especially for the range $[150,200]$. In addition, we use the measures proposed by Griffiths~\etal~\cite{Griffiths2004} and Arun~\etal~\cite{Arun2010} for the same models to compare the results. For each number of topics, we generate three models and report the average across these models.

\speedupfigure{
\input{figures/LODCat-topic-quality-CP-small}
}{fig:se-lodcat-topic-quality-CP-small}

\speedupfigure{
\input{figures/LODCat-topic-quality-griffiths-small}
}{fig:se-lodcat-topic-quality-griffiths-small}

\speedupfigure{
\input{figures/LODCat-topic-quality-arun-small}
}{fig:se-lodcat-topic-quality-arun-small}

Figure~\ref{fig:se-lodcat-topic-quality-CP-small} shows the results of the coherence measure as box plots. The average coherence value increases with the number of topics until it reaches a plateau with around 150 topics. All models with less than 400 topics seem to have a similar coherence value. From 400 topics on, the average coherence value of the models drops. To ensure that this observation is not solely based on a single model, we added models with 500, 750 and 1000 topics.\footnote{We do not report the values of $\cprob{\s{documents}}{\s{wordDists}}$ and $\s{measureArun}$ for these models since the evaluation of the large models consumed too much RAM.} They confirm the observation.
Figures~\ref{fig:se-lodcat-topic-quality-griffiths-small} and~\ref{fig:se-lodcat-topic-quality-arun-small} show the values of $\cprob{\s{documents}}{\s{wordDists}}$ and $\s{measureArun}$, respectively. Both values increase with the number of topics. Hence, the probability $\cprob{\s{documents}}{\s{wordDists}}$ would choose the topic model with 400 topics as best model. The $\s{measureArun}$ measure instead would choose the topic model with 10 topics. These observations confirm the results from our experiment in Section~\ref{sec:se-results-exp1}.


\section{Questionnaire}
\label{sec:ap-lodcat-questionnaire}

This section shows the 10 questions of the LODCat questionnaire of the third experiment described in Section~\ref{sec:se-evaluation}. All questions in the questionnaire have the same structure. An example of these questions is the following:

\emph{Given the dataset 0ca, which of the following 4 topics does not fit to the dataset?}

In the online questionnaire, the name of the dataset in the question is linked to the \gls{rdf} file to give the user access to the dataset. The question is followed by the list of topics. The topics have been ordered randomly when creating the questions. We list the topics in this order, but mark the meaning of the topics, i.e, whether their are 1st, 2nd, 3rd or the intruder topic (I). Each topic comes with a title and 10 top words.

\begin{enumerate}
\item Dataset \url{https://hobbitdata.informatik.uni-leipzig.de/lodcat/lodalot/evaluation/rdf/0ca881a1fc5817a3575d2ff6191cc622%3Ftype=hdt.n3}
\begin{itemize}
\item[I] \textbf{College basketball}\\
state, basketball, conference, university, ncaa, kentucky, carolina, tournament, man, college
\item[2nd] \textbf{Philosophy}\\
language, study, social, theory, university, work, culture, history, press, philosophy
\item[3rd] \textbf{Information}\\
use, system, software, user, datum, computer, include, information, support, service
\item[1st] \textbf{University}\\
university, college, research, science, institute, professor, award, work, study, society
\end{itemize}
\vspace{1cm}

\item Dataset \url{https://hobbitdata.informatik.uni-leipzig.de/lodcat/lodalot/evaluation/rdf/0df3609cdd77b40d2a06ea88d09f656b%3Ftype=hdt.n3}
\begin{itemize}
\item[1st] \textbf{Nausea}\\
disease, patient, may, cause, treatment, use, cancer, blood, symptom, include
\item[I] \textbf{Train}\\
railway, station, line, train, service, locomotive, rail, passenger, bus, transport
\item[2nd] \textbf{Philosophy}\\
language, study, social, theory, university, work, culture, history, press, philosophy
\item[3rd] \textbf{Football}\\
player, footballer, football, expatriate, fc, people, bear,
play, birth, club
\end{itemize}
\vspace{1cm}

\item Dataset \url{https://hobbitdata.informatik.uni-leipzig.de/lodcat/lodalot/evaluation/rdf/6f3341506ee83bd28b787c8e2f6858c1%3Ftype=hdt.n3}
\begin{itemize}
\item[1st] \textbf{Information}\\
use, system, software, user, datum, computer, include, information, support, service
\item[2nd] \textbf{Physicist}\\
prize, physics, award,
design, mitchell, research, science, physicist, quantum, brain
\item[3rd] \textbf{University}\\
university, college, research, science, institute, professor, award, work, study, society
\item[I] \textbf{Sweden}\\
danish, norwegian, norway, denmark, club, swedish, sweden, copenhagen, sc, people
\end{itemize}
\vspace{1cm}

\item Dataset \url{https://hobbitdata.informatik.uni-leipzig.de/lodcat/lodalot/evaluation/rdf/c89db4032b3809c7ef9e57e5485ca2f8%3Ftype=hdt.n3}
\begin{itemize}
\item[I] \textbf{Genus}\\
species, genus,
describe, family, find, name, fish, genera, plant, america
\item[3rd] \textbf{Sir}\\
john, london, son, king, william, british, sir,
english, die, death
\item[2nd] \textbf{Building}\\
building, house, new, park, street, area, south, build, town, centre
\item[1st] \textbf{University}\\ 
university, college, research, science, institute, professor, award, work, study, society
\end{itemize}
\vspace{1cm}

\item Dataset \url{https://hobbitdata.informatik.uni-leipzig.de/lodcat/lodalot/evaluation/rdf/df7e479638564102139dc1ff359ebc15%3Ftype=hdt.n3}
\begin{itemize}
\item[2nd] \textbf{University}\\
university,
college, research, science, institute, professor, award, work, study, society
\item[1st] \textbf{Information}\\
use, system,
software, user, datum, computer, include, information, support, service
\item[I] \textbf{Flower}\\
plant, bird, flower, species,
leaf, long, flora, grow, name, tree
\item[3rd] \textbf{Atlantic}\\
album, song, release, band, music, chart, record, single, track,
records
\end{itemize}
\vspace{1cm}

\item Dataset \url{https://hobbitdata.informatik.uni-leipzig.de/lodcat/lodalot/evaluation/rdf/113e514ef0a94b738ce37350525cae0d%3Ftype=hdt.n3}
\begin{itemize}
\item[2nd] \textbf{Portugal}\\
airport,
international, brazil, portuguese, são, romanian, portugal, brazilian, language, romania
\item[1st] \textbf{China}\\
chinese,
china, singapore, li, wang, shanghai, chen, beijing, hong, zhang
\item[I] \textbf{Season}\\
game, team, season, win, first,
league, play, point, second, record
\end{itemize}
\vspace{1cm}

\item Dataset \url{https://hobbitdata.informatik.uni-leipzig.de/lodcat/lodalot/evaluation/rdf/1d6c4d4a80a507331801dda33f631097%3Ftype=hdt.n3}
\begin{itemize}
\item[1st] \textbf{Upland}\\
river, island,
area, mountain, park, north, south, forest, water, land
\item[I] \textbf{Govinda (actor)}\\
film, indian, tamil, singh, role,
kumar, actor, hindi, award, telugu
\item[2nd] \textbf{Portugal}\\
airport, international, brazil, portuguese, são, romanian,
portugal, brazilian, language, romania
\end{itemize}
\vspace{1cm}

\item Dataset \url{https://hobbitdata.informatik.uni-leipzig.de/lodcat/lodalot/evaluation/rdf/b056341b71e7ecb9ed12f144c085dc60%3Ftype=hdt.n3}
\begin{itemize}
\item[I] \textbf{Painting}\\
art, museum,
work, artist, painting, gallery, exhibition, painter, collection, new
\item[1st] \textbf{Germany}\\
german, der, die, und, germany,
flag, berlin, austrian, vienna, work
\item[2nd] \textbf{Philosophy}\\
language, study, social, theory, university, work, culture,
history, press, philosophy
\end{itemize}
\vspace{1cm}

\item Dataset \url{https://hobbitdata.informatik.uni-leipzig.de/lodcat/lodalot/evaluation/rdf/79d5ea832dd19745b7707644af618ef1%3Ftype=hdt.n3}
\begin{itemize}
\item[2nd] \textbf{Portugal}\\
airport,
international, brazil, portuguese, são, romanian, portugal, brazilian, language, romania
\item[I] \textbf{American hockey league}\\
hockey, player, season, team, ice, toronto, nhl, league, new, play
\item[1st] \textbf{Business}\\
company, bank,
business, million, market, financial, industry, year, group, tax
\end{itemize}
\vspace{1cm}

\item Dataset \url{https://hobbitdata.informatik.uni-leipzig.de/lodcat/lodalot/evaluation/rdf/979c3e5c7aa2f1937a6939ec9e16b965%3Ftype=hdt.n3}
\begin{itemize}
\item[1st] \textbf{Upland}\\
river, island,
area, mountain, park, north, south, forest, water, land
\item[2nd] \textbf{Australia}\\
australia, australian, south, sydney, wales,
melbourne, victoria, new, queensland, western
\item[I] \textbf{Uttar pradesh}\\
india, indian, state, delhi, pradesh,
bangladesh, bengal, tamil, national, nepal
\end{itemize}
\end{enumerate}

